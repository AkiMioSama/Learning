# Sled Notes 01

开始读读sled的源码。顺便学习一下怎么写rust

代码仓库在[这里](https://github.com/spacejam/sled)，貌似已经有一段时间没有更新了

sled聚合了一些很nb的技术，从[这里](https://github.com/spacejam/sled/wiki/sled-architectural-outlook)可以看到，bw-tree作为index，下面的cache层是参考了LLAMA，并发控制参考了Cicada，而存储层有一些lsmtree，以及kv分离的参考文章，所以可能是通过lsmtree做的，内存对象的回收则是和bwtree一样是EBR。

所以看看这个项目学习rust的同时，也可以学习一下上述技术的具体实现。可谓是一举多得了。

还是最经典的，先从写入链路看起。这里我们就从Tree的写入看起，因为Tree的写入是sled的核心链路。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912150649.png)

谨记顺便学习写法。K和V都是泛型，其中K要求实现`AsRef<u8>`，这个比较好理解，就是转化成slice。而V要求可以转换成`IVec`。这里注意Rust的Into和From是对称的，实现了其中一个另一个也会实现。

我们看一下这里的IVec是什么，在ivec.rs中。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912151015.png)

其中SZ是`sizeof(usize)`，也就是8。所以IVec中的数据要么是一个inline的数据，要么是一个指针指向远端的数据。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912151221.png)

构造函数传入的都是一个slice。如果slice小于等于cutoff，也就是7byte的话，就会inline存储。并且最后一位作为trailer，会存储长度等元数据。

否则的话就会作为远端的指针。存有具体的数据以及RemoteHeader。其中Header的作用是存有一个rc以及数据的长度。然后把slice拷贝到一个新分配的空间中。由于是新分配的，所以不可能有overlap，用的是`copy_nonoverlapping`。然后将这个指针写入到IVec中。由于是align的8byte，所以最后3位一定是0。IVec也会通过最后一位来判断当前的数据是否是inline的。

对于Clone来说，如果是inline的话，直接把8byte拷贝过去，否则的话就会在刚才的header中原子加1。drop的时候也是原子减，当rc为0的时候就会释放掉这段内存。不过他这里的memory fence用的有点诡异。而且感觉这里会有并发问题，并发的drop和clone，可能会导致错误的释放，但是他这里貌似没有实现Sync，所以貌似没啥问题。

然后回到tree中。我们先跳过事务相关的部分，先看看一次写入是怎么落盘的。

调用`insert_inner`。这里有一个先查找的逻辑

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912155605.png)

view这里可以猜测是查找该key对应的数据。返回的是NodeView和pid。NodeView是key对应的page，pid猜测是对应的key的pid

然后通过node view以及key找到对应的value。判断如果value如果相同的话，则说明已经成功插入。我们就会直接返回。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912155910.png)

否则的话，则会创建一个link。这个link对应的就是bwtree的一个delta。

如果value是none的话，代表删除，就插入一个删除的delta，存的是key。否则的话就插入一个Set的delta，存的是kv。

然后将delta安装到tree中。如果成功的话就返回成功，这里的link是cas的结果。失败的话就是冲突了。我们就要重试。这里的result中就包了一个Conflictable。所以返回给外面的话，如果是link或者view for key失败了，外部会返回失败，退出重试循环。否则如果只是conflict的话会重试insert inner。

包两层Result的原因感觉是为了处理可重试错误和不可重试错误。对于可重试错误，我们就loop，不可重试错误的话，则直接返回具体的error。

然后看看刚才的两个比较关键的函数，分别是link和view for key。对应了读和写。

先看link。传入的参数分别是刚才读到的pid，node view，以及刚才的delta

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912180121.png)

先把page view转化成Node，然后调用apply，应用本次的变更。

然后判断如果老节点超过了delta的threshold，就通过CAS来尝试把新的节点换入，成功的话本次就是一次consolidation。失败的话就重试。

可以看到如果发现需要consolication的时候，他会强行执行。而不会说等到本次执行结束后在尝试consolidate。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912180756.png)

和原文还是一致的，这里会先apply，再做consolidation，就是不确定他这里的cache_info.len()的计算有没有算到新apply的那个delta。

接着往下走，基于最新的Node创建一个新的page。这里会进入一个贼长的循环。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912181033.png)

先申请log buffer，然后获取lsn。

下面有一个获取ts，以及一段注释，我暂时还没看懂。。之后再一块看。

申请一个CacheInfo的vector，然后把新的cache info给append上去。其中CacheInfo中存的就是ts，lsn，以及一个disk pointer。看代码应该是用来指向WAL的区域的。因为这里是获取的log_reservation.pointer

更新new page的cache info为刚才申请的那个vector。（目前还不太清楚为什么不共用而是copy一份出来，如果是immutable的话感觉做成块状链表会更好一些）

然后通过cas把刚才的new page换入到old page的page view中。等下我们去看一下这些结构的关系。

得到CAS的结果之后，下面就会判断，如果本次交换成功了。那么我们就可以把旧的page view回收掉。

更新lru，因为本次成功后会使得page变得更大。然后增加link count。

失败的话，则会放弃掉本次log reservation。然后获取到当前最新的ts。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912194228.png)

如果为old ts，说明link失败的原因是数据移动了。否则的话说明有其他的更新。有其他的更新我们就会在外部重试，即最外面的insert inner中。而如果只是数据移动的话则在内部重试。

到这里基本的链路就梳理清楚了。然后应该看一下数据结构之间的关系，以及刚才没有深入去看的几个函数，比如node的apply，获取node view，以及node kv pair等。

先看一下数据结构

```rust
pub(crate) struct View<'g> {
    pub node_view: NodeView<'g>,
    pub pid: PageId,
}
pub struct NodeView<'g>(pub(crate) PageView<'g>);
pub struct PageView<'g> {
    pub(in crate::pagecache) read: Shared<'g, Page>,	// shared是epoch gc保护的指针
    pub(in crate::pagecache) entry: &'g Atomic<Page>,	// Atomic就是原子指针
}
// shared里的phantom data用来标识这个共享指针的生命周期，并且数据是*const T类型的
pub(crate) struct Shared<'g, T: 'g + ?Sized + Pointable> {
    data: usize,
    _marker: PhantomData<(&'g (), *const T)>,
}
// atomic中表示拥有*mut T，具有所有权，所以不需要引用，也就不需要标注数据的生命周期。
pub(crate) struct Atomic<T: ?Sized + Pointable> {
    data: AtomicUsize,
    _marker: PhantomData<*mut T>,
}
pub struct Page {
    update: Option<Update>,
    cache_infos: Vec<CacheInfo>,
}
pub(in crate::pagecache) enum Update {
    Link(Link),
    Node(Node),
    Free,
    Counter(u64),
    Meta(Meta),
}
// ts标识page有无改变。每次变更都会修改ts
pub struct CacheInfo {
    pub ts: u64,
    pub lsn: Lsn,
    pub pointer: DiskPtr,
}
// disk ptr指向内存的buffer，或者是文件中的某个位置
pub enum DiskPtr {
    /// Points to a value stored in the single-file log.
    Inline(LogOffset),
    /// Points to a value stored off-log in the heap.
    Heap(Option<NonZeroU64>, HeapId),
}
// 一个结点，也是一个page。
// overlay中存的是modification，或者删除（对应value为None）
pub struct Node {
    // the overlay accumulates new writes and tombstones
    // for deletions that have not yet been merged
    // into the inner backing node
    pub(crate) overlay: im::OrdMap<IVec, Option<IVec>>,
    pub(crate) inner: Arc<Inner>,
}
// inner则是一堆字节。
pub struct Inner {
    ptr: *mut UnsafeCell<[u8]>,
}
// meta，这里代表的是非叶节点。
pub struct Meta {
    pub(crate) inner: BTreeMap<IVec, PageId>,
}
// link，表示新的修改。
pub(crate) enum Link {
    /// A new value is set for a given key
    Set(IVec, IVec),
    /// The kv pair at a particular index is removed
    Del(IVec),
    /// A child of this Index node is marked as mergable
    ParentMergeIntention(PageId),
    /// The merging child has been completely merged into its left sibling
    ParentMergeConfirm,
    /// A Node is marked for being merged into its left sibling
    ChildMergeCap,
}
// page cache
pub struct PageCache(Arc<PageCacheInner>);
// 有一个page table，一个用来用来分配pid的mutex。一个freelist，用来管理释放的pid。
// Lru管理缓存，一个Log，下面的东西不太清楚之后再看看
pub struct PageCacheInner {
    was_recovered: bool,
    pub(crate) config: RunningConfig,
    inner: PageTable,
    next_pid_to_allocate: Mutex<PageId>,
    // needs to be a sub-Arc because we separate
    // it for async modification in an EBR guard
    free: Arc<Mutex<FastSet8<PageId>>>,
    #[doc(hidden)]
    pub log: Log,
    lru: Lru,

    idgen: AtomicU64,
    idgen_persists: AtomicU64,
    idgen_persist_mu: Mutex<()>,

    // fuzzy snapshot-related items
    snapshot_min_lsn: AtomicLsn,
    links: AtomicU64,
    snapshot_lock: Mutex<()>,
}
// running config应该是系统运行的环境。其中config就是一堆参数，file则是std::fs的file
pub struct RunningConfig {
    inner: Config,
    pub(crate) file: Arc<File>,
    pub(crate) heap: Arc<Heap>,
}
// heap则是特殊使用的slab分配器。目前理解他是一个堆就行。可以提供alloc和dealloc
pub(crate) struct Heap {
    // each slab stores
    // items that are double
    // the size of the previous,
    // ranging from 64k in the
    // smallest slab to 2^48 in
    // the last.
    slabs: [Slab; 32],
}
// 这个page table是个树形的结构，而非哈希表。索引的时候可能根据pid就直接找到了。可以理解成是2级页表。
pub struct PageTable {
    head: Atomic<Node1>,
}
struct Node1 {
    children: [Atomic<Node2>; NODE1_FAN_OUT],
}
struct Node2 {
    children: [Atomic<Page>; NODE2_FAN_OUT],
}
// log，下来再看
pub struct Log {
    /// iobufs is the underlying lock-free IO write buffer.
    pub(crate) iobufs: Arc<IoBufs>,
    pub(crate) config: RunningConfig,
}
```

然后来看看view for key

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912212717.png)

这里注释写的比较清楚。我们在遇到SMO的时候会辅助完成。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912212816.png)

一开始初始化了一堆变量，以及一个用来retry的宏。

可以看到cursor表示的就是当前的page id。而smo budget表示最大帮助做SMO的次数。并且在retry的时候会重新走到root中，重置这些状态。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912213226.png)

view for pid根据pid获取view。如果获取失败的话就会返回。获取成功的话这里有个判断。确保merging child是空的。因为view for pid会处理merging child。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912213422.png)

这里看一下思路。首先通过pid从pagecache中获取到NodeView。

获得view之后，判断一下这个page上有没有merging child。如果有的话就帮助他merge。最后返回这个page view。

这里解引用有个很长的链路。View的Deref是Node

```rust
impl<'g> Deref for View<'g> {
    type Target = Node;

    fn deref(&self) -> &Node {
        &*self.node_view
    }
}
```

这个Node是通过对node view解引用获得的。

```rust
impl<'g> Deref for NodeView<'g> {
    type Target = Node;
    fn deref(&self) -> &Node {
        self.0.as_node()
    }
}
```

然后是通过page view的`as_node`

然后是调用update的`as_node`

```rust
fn as_node(&self) -> &Node {
  	match self {
    	Update::Node(node) => node,
    	other => panic!("called as_node on non-Node: {:?}", other),
  	}
}
```

把最终update中的node返回回去。

这时候回来的是&Node，没有访问到我们想访问的merge child，所以继续解引用。这里会走到`Inner`中，也就是那段字节中。继续解引用得到的是`Header`

```rust
impl Deref for Inner {
    type Target = Header;

    fn deref(&self) -> &Header {
        self.header()
    }
}
```

这个header中存了很多的数据。包括很多数据的长度等

我们先回到view for pid中。了解了获取page的时候会顺便帮忙merge一下就行。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912222857.png)

然后是一些double check。

如果发现overshot的话，说明是中途来了merge request。如果是undershot的话，说明有了split，我们应该跟着blink tree的操作，找右节点。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912223343.png)

这里找到右节点，设置cursor，然后记录一下unsplit parent。如果没有parent的话，说明是split root。

当没有undershot，即当前节点没有分裂的时候，我们会判断一下unsplit parent

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912223629.png)

我们现在要做的就是把右孩子插入回去。之所以不在上一步做是因为我们还需要获取当前view，以便获取low key。所以复用了前面获取view的代码。

然后把新的变更安装回去，让本次split生效。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912224127.png)

然后下一步我们判断，如果我们还可以继续做SMO，并且当前page应该merge了，而且具有parent。我们就会判断一下是否可以merge（看起来没有redistribution），然后在parent上安装这个link。并且调用parent的merge node完成merge。注意还要判断的是保证不是最左的branch，因为merge只能向左merge。

这里看起来是一个一步的操作，但实际上link中的apply中，我们会创建merge child，这样如果有其他线程过来，就可以帮助我们做merge。

![](https://picsheep.oss-cn-beijing.aliyuncs.com/pic/20220912224653.png)

然后判断如果是index，即meta，则根据key找到下一个位置。更新cursor，parent view，以及是否是最左分支。

否则的话则会返回view。

可以看到读路径和正常的btree不同的就是需要处理很多SMO相关的逻辑。

这一节就先到这里。

