Flusher会切换iobuf，把前一个封上，然后写入到磁盘中

Flusher -> roll_iobuf -> maybe_seal_and_write_iobuf

maybe_seal_and_write_io_buf: 

* 获取下个iobuf的起始lsn
* 构建新的iobuf。并写入segment header
* 换入新的iobuf
* 写入log -> write_to_log

write_to_log:

* 序列化log，写入，fsync
* mark interval
* 延迟推进max_header_stable_lsn到stored_max_stable_lsn
* sa_stabilize到current_max_header_stable_lsn

从RCU的角度来理解这个延迟推进。这里是希望所有的reader都看到了本次的刷入。这样保证没有其他的读者还在复用之前的log segment。在这之后就可以推进stable lsn，SegmentAccountant就可以释放掉之前的log segment了。

这里interval就是linkbuffer，标记interval，连续后就可以推进stable lsn了。

sa.stabilize:

* 找到从上次stable lsn到本次stable lsn之间的所有segment
* deactivate_segment
* 计算free log segment的比例。过多时将inactive segment设置成draining。并放入到segment cleaner中

sa.next:

* 从free的log segment中找到一个复用
* 找不到就bump file。
* 初始化新的log segment
* 构建lsn到log segment的映射。

之所以有是否bump file，是因为文件系统提供sync_all和sync_data。如果我们bump file了，需要同样同步metadata，就要使用sync_all。否则的话使用sync_data就行。

当我们使用cache提供的replace接口的时候，会使得之前的数据失效。比如之前的heap item，之前的log。所以在replace的时候我们会在这个segment中标记需要回收的历史数据。在切换到inactive的时候，说明当前的segment已经全局可见了。我们就可以清理掉历史数据了。

在segment的active_to_inactive中。我们会清理那些过期的heap item。并在完成后尝试通过possibly_clean_or_free_segment清理掉刚刚deactive的segment。

possibly_clean_or_free_segment:

* 计算当前inactive segment的live_pct。即有效的数据的比例。超过一定比例的时候就开启搬运数据
* 对于搬运完毕的segment，我们可以切换到free的状态。找到最大的覆盖当前segment数据的lsn。定位到他的segment。并在这个segment变成inactive的时候，释放掉当前的segment。因为后续的读者会看到搬运后的数据。

segment cleaner会周期性的取出offset最小的segment中的某一个pid。重写对应的page。调用的也是cas_page，和对上层提供的replace调用的是一样的。